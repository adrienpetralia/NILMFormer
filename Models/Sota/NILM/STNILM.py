import math
import torch

from torch import nn

from Models.Sota.Layers.SwitchFFN import FeedForward, SwitchFeedForward, clone_module_list
from Models.Sota.NILM.BERT4NILM import PositionalEmbedding, MultiHeadedAttention



class SublayerConnection(nn.Module):
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        

    def forward(self, x, sublayer):
        return self.layer_norm(x + self.dropout(sublayer(x)))


class TransformerBlock(nn.Module):
    def __init__(self, hidden, attn_heads, 
                 n_experts, 
                 feed_forward_hidden, 
                 dropout):
        super().__init__()
        self.attention    = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)
        self.feed_forward = SwitchFeedForward(d_model=hidden, expert=FeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout), n_experts=n_experts)

        self.norm1    = nn.LayerNorm(hidden)
        self.dropout1 = nn.Dropout(dropout)
        self.norm2    = nn.LayerNorm(hidden)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)
        

    def forward(self, x, mask=None):
        new_x = self.attention(x, x, x, mask=mask)
        x     = torch.add(x, new_x)
        x     = self.norm1(x)
        x     = self.dropout1(x)
        
        new_x, counts, route_prob, n_dropped, route_prob_max = self.feed_forward(x)
        x     = torch.add(x, new_x)
        x     = self.norm2(x)
        x     = self.dropout2(x)

        return x, counts, route_prob, n_dropped, route_prob_max


class STNILM(nn.Module):
    def __init__(self, window_size, 
                 c_in=1, c_out=1, 
                 n_experts=10, 
                 dp_rate=0.1,
                 weight_moe=0.1, criterion=nn.MSELoss()):
        super().__init__()

        self.latent_len   = int(window_size / 2)
        self.dropout_rate = dp_rate

        self.n_experts   = n_experts
        self.criterion   = criterion
        self.weight_moe  = weight_moe
        self.output_size = c_out

        self.hidden    = 256
        self.heads     = 2
        self.n_layers  = 2

        self.conv = nn.Conv1d(in_channels=c_in, out_channels=self.hidden, kernel_size=5, stride=1, padding=2, padding_mode='replicate')
        self.pool = nn.LPPool1d(norm_type=2, kernel_size=2, stride=2)

        self.position   = PositionalEmbedding(max_len=self.latent_len, d_model=self.hidden)
        self.layer_norm = nn.LayerNorm(self.hidden)
        self.dropout    = nn.Dropout(p=self.dropout_rate)

        transformer_layer       = TransformerBlock(self.hidden, self.heads, self.n_experts, self.hidden * 4, self.dropout_rate)
        self.transformer_blocks = clone_module_list(transformer_layer, self.n_layers)

        self.deconv  = nn.ConvTranspose1d(in_channels=self.hidden, out_channels=self.hidden, kernel_size=4, stride=2, padding=1)
        self.linear1 = nn.Linear(self.hidden, 128)
        self.linear2 = nn.Linear(128, self.output_size)

        self.truncated_normal_init()

    def truncated_normal_init(self, mean=0, std=0.02, lower=-0.04, upper=0.04):
        params = list(self.named_parameters())
        for n, p in params:
            if 'layer_norm' in n:
                continue
            else:
                with torch.no_grad():
                    l = (1. + math.erf(((lower - mean) / std) / math.sqrt(2.))) / 2.
                    u = (1. + math.erf(((upper - mean) / std) / math.sqrt(2.))) / 2.
                    p.uniform_(2 * l - 1, 2 * u - 1)
                    p.erfinv_()
                    p.mul_(std * math.sqrt(2.))
                    p.add_(mean)

    def forward(self, x):
        # Input as B, C, L
        x_token   = self.pool(self.conv(x)).permute(0, 2, 1)
        embedding = x_token + self.position(x)
        x = self.dropout(self.layer_norm(embedding))

        # Mixture Of Expert router
        counts, route_prob, n_dropped, route_prob_max = [], [], [], []
        for layer in self.transformer_blocks:
            x, f, p, n_d, p_max = layer(x)
            counts.append(f)
            route_prob.append(p)
            n_dropped.append(n_d)
            route_prob_max.append(p_max)

        x = self.deconv(x.permute(0, 2, 1)).permute(0, 2, 1)
        x = torch.tanh(self.linear1(x))
        x = self.linear2(x)

        if self.training:
            counts     = torch.stack(counts)
            route_prob = torch.stack(route_prob)
            route_prob_max =  torch.stack(route_prob_max) 

            total = counts.sum(dim=-1, keepdims=True)
            # Fraction of tokens routed to each expert
            route_frac = counts / total
            # Mean routing probability
            route_prob = route_prob / total
            # Load balancing loss
            loss_moe = self.n_experts * (route_frac * route_prob).sum()

            return x.permute(0, 2, 1), loss_moe
        else:
            return x.permute(0, 2, 1)
    
    def train_one_epoch(self, loader, optimizer, device='cuda'):
        """
        Train STNILM for one epoch
        """
        self.train()
        total_loss = 0
        
        for seqs, labels, status  in loader:
            seqs, labels, status = torch.Tensor(seqs.float()).to(device), torch.Tensor(labels.float()).to(device), torch.Tensor(status.float()).to(device)
            
            # Forward model
            optimizer.zero_grad()
            power_logits, loss_moe = self.forward(seqs)
        
            loss = self.criterion(power_logits, labels)
            loss = loss + self.weight_moe * loss_moe
            
            loss.backward()
            optimizer.step() 
            total_loss += loss.item()
           
        total_loss = total_loss / len(loader)

        return total_loss